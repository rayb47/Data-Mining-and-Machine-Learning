{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7aef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e29a4c",
   "metadata": {},
   "source": [
    "## Common Functions (used across more than 1 task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722a5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    Loads data from the respective given datasets, and imputes values if necessary.\n",
    "    \n",
    "    Parameters:\n",
    "        - None\n",
    "        \n",
    "    Returns:\n",
    "        - None\n",
    "    '''\n",
    "    # Loads of data\n",
    "    x_train = np.load('x_train.npy')\n",
    "    x_test = np.load('x_test.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    y_test = np.load('y_test.npy')\n",
    "    \n",
    "    # Summarizes missing values\n",
    "    train_nans, train_infs = summarize_missing_values(x_train, 'x_train')\n",
    "    test_nans, test_infs = summarize_missing_values(x_test, 'x_test')\n",
    "    \n",
    "    # Imputation of data (if necessary)\n",
    "    if train_nans > 0 or train_infs > 0:\n",
    "        x_train = impute_data(x_train)\n",
    "    if test_nans > 0 or test_infs > 0:\n",
    "        x_test = impute_data(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137e73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_missing_values(data, name):\n",
    "    '''\n",
    "    Summarizes the missing contents of the dataset (if any)\n",
    "    \n",
    "    Parameters:\n",
    "        - data: Dataset\n",
    "        - name: Name of the dataset\n",
    "    '''\n",
    "    num_nans = np.isnan(data).sum()\n",
    "    num_infs = np.isinf(data).sum()\n",
    "    print(f\"{name} - Total NaNs: {num_nans}, Total Infs: {num_infs}\")\n",
    "    return num_nans, num_infs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc121e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(data):\n",
    "    '''\n",
    "    Imputes missing values in the dataset (if any)\n",
    "    \n",
    "    Parameters:\n",
    "        - data: Dataset\n",
    "        \n",
    "    Returns:\n",
    "        - imputer.fit_transform(data): Transformed imputed data\n",
    "    '''\n",
    "    # Replace Inf values with NaN\n",
    "    data[np.isinf(data)] = np.nan\n",
    "    # Uses median to reduce the impact of outliers\n",
    "    imputer = SimpleImputer(strategy='median')  \n",
    "    return imputer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831eb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train_data, test_data=None, minmax=False, lower_lim=0, upper_lim=1):\n",
    "    '''\n",
    "    Scales data using the scaler fitted on the training set\n",
    "    \n",
    "    Parameters:\n",
    "        - train_data: Training dataset\n",
    "        - test_data: Testing dataset\n",
    "        - minmax: Decides whether to use the standard scaler or MinMaxScaler\n",
    "        - lower_lim: Lower limit for the MinMax scaler\n",
    "        - upper_lim: Upper limit for the MinMax scaler\n",
    "        \n",
    "    Returns:\n",
    "        - train_scaled: Scaled training data\n",
    "        - test_scaled: Scaled testing data\n",
    "        - scaler: Scaler object\n",
    "    '''\n",
    "    if minmax == False:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler(feature_range=(lower_lim, upper_lim))\n",
    "        \n",
    "    train_scaled = scaler.fit_transform(train_data)\n",
    "    \n",
    "    if test_data is None:\n",
    "        test_scaled = None\n",
    "    else:\n",
    "        test_scaled = scaler.transform(test_data)\n",
    "\n",
    "    return train_scaled, test_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d69a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve_multiclass(y_test, probabilities, classes):\n",
    "    '''\n",
    "    Plots a precision-recall curve for a multi-class classification problem\n",
    "    \n",
    "    Parameters:\n",
    "        - y_test: Target test dataset\n",
    "        - probabilities: Contains stored probability scores for all classes\n",
    "        - classes: Unique classes in the target dataset\n",
    "        \n",
    "    Returns:\n",
    "        - None\n",
    "    '''\n",
    "    # Binarize the output classes in a one-vs-all fashion\n",
    "    y_test_binarized = label_binarize(y_test, classes=classes)\n",
    "\n",
    "    # Setup colormap\n",
    "    cmap = get_cmap(\"tab20\") \n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(classes))]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # Plots each of the curves\n",
    "    for i, color in enumerate(colors):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], probabilities[:, i])\n",
    "        average_precision = average_precision_score(y_test_binarized[:, i], probabilities[:, i])\n",
    "        plt.plot(recall, precision, color=color, linestyle='-', marker='.', label=f'Class {classes[i]} (AP={average_precision:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve per class')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e38dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, y_test):\n",
    "    '''\n",
    "    Plots a confusion matrix\n",
    "    \n",
    "    Parameters:\n",
    "        - cm: Confusion Matrix object\n",
    "        - y_test: Target test dataset\n",
    "        \n",
    "    Returns:\n",
    "        - None\n",
    "    '''\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, np.unique(y_test), rotation=45)\n",
    "    plt.yticks(tick_marks, np.unique(y_test))\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    # Annotates squares with the numeric values\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
